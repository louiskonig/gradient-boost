{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>NO</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8576</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8579</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8580</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8581</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8593</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8147 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  gndr  \\\n",
       "0        6    3.0      3.0     10.0     5.0    8.0      5.0     4.0   2.0   \n",
       "1        6    6.0      5.0      7.0     5.0    9.0      3.0     2.0   2.0   \n",
       "2        6    1.0      8.0      8.0     8.0    7.0      6.0     3.0   1.0   \n",
       "3        6    4.0      6.0      6.0     7.0   10.0      6.0     2.0   2.0   \n",
       "4        6    5.0      6.0      7.0     5.0    8.0      7.0     2.0   2.0   \n",
       "6        6    3.0      0.0      5.0     2.0    0.0      2.0     2.0   1.0   \n",
       "7        6    2.0      4.0      5.0     3.0   10.0      5.0     2.0   2.0   \n",
       "8        6    2.0      8.0      8.0     8.0    9.0      6.0     4.0   2.0   \n",
       "9        6    4.0      4.0      4.0     8.0    7.0      4.0     2.0   2.0   \n",
       "10       6    1.0      6.0      7.0     7.0    9.0      5.0     2.0   2.0   \n",
       "11       6    4.0      5.0      7.0     7.0    7.0      5.0     3.0   1.0   \n",
       "12       6    4.0      7.0      7.0     4.0    9.0      6.0     2.0   1.0   \n",
       "13       6    2.0      1.0      9.0     7.0    8.0      4.0     3.0   1.0   \n",
       "14       6    4.0      4.0      5.0     3.0    8.0      1.0     2.0   2.0   \n",
       "15       6    5.0      4.0      5.0     5.0    9.0      3.0     3.0   2.0   \n",
       "16       6    4.0      5.0      7.0     7.0    8.0      3.0     3.0   1.0   \n",
       "17       6    2.0      7.0      7.0     7.0    6.0      6.0     3.0   1.0   \n",
       "18       6    0.0      9.0      7.0     8.0    9.0      5.0     4.0   1.0   \n",
       "19       6    2.0      6.0      6.0     6.0    8.0      4.0     3.0   2.0   \n",
       "20       6    7.0      3.0      4.0     4.0    7.0      6.0     4.0   1.0   \n",
       "21       6    1.0      8.0      8.0     6.0    8.0      7.0     4.0   1.0   \n",
       "22       6    2.0      7.0      7.0     8.0    9.0      6.0     3.0   1.0   \n",
       "23       6    2.0      4.0      8.0     8.0    9.0      6.0     2.0   2.0   \n",
       "24       6    0.0      7.0      7.0     4.0    9.0      4.0     2.0   2.0   \n",
       "25       6    0.0      6.0      7.0     5.0    7.0      6.0     2.0   2.0   \n",
       "26       6    2.0      7.0      6.0     5.0    8.0      4.0     3.0   2.0   \n",
       "27       6    0.0      5.0      8.0     6.0    9.0      4.0     2.0   2.0   \n",
       "28       6    2.0      8.0      8.0     8.0   10.0      6.0     3.0   2.0   \n",
       "29       6    7.0      8.0      5.0     5.0    8.0      5.0     3.0   2.0   \n",
       "30       6    6.0      8.0      8.0     3.0    5.0      6.0     1.0   2.0   \n",
       "...    ...    ...      ...      ...     ...    ...      ...     ...   ...   \n",
       "8564     7    2.0      1.0      5.0     4.0   10.0      7.0     4.0   1.0   \n",
       "8565     7    5.0      5.0      3.0     4.0    5.0      6.0     1.0   2.0   \n",
       "8566     7    2.0      7.0      8.0     6.0    8.0      7.0     3.0   2.0   \n",
       "8567     7    2.0      4.0      8.0     7.0    9.0      7.0     4.0   2.0   \n",
       "8568     7    1.0      2.0      8.0     6.0    9.0      7.0     3.0   2.0   \n",
       "8569     7    6.0      5.0      3.0     4.0    9.0      6.0     4.0   1.0   \n",
       "8570     7    1.0      5.0      7.0     7.0    8.0      7.0     4.0   1.0   \n",
       "8571     7    3.0      4.0      9.0     7.0    8.0      6.0     3.0   1.0   \n",
       "8572     7    5.0      7.0      8.0     5.0    8.0      3.0     1.0   1.0   \n",
       "8573     7    1.0      8.0      8.0     6.0    9.0      7.0     3.0   2.0   \n",
       "8574     7    1.0      4.0      5.0     6.0    7.0      7.0     3.0   2.0   \n",
       "8575     7    3.0      8.0      7.0     7.0    8.0      7.0     3.0   1.0   \n",
       "8576     7    4.0      5.0      4.0     4.0    7.0      6.0     3.0   1.0   \n",
       "8577     7    6.0      6.0      9.0     7.0    5.0      4.0     2.0   2.0   \n",
       "8578     7    1.0      5.0      5.0     5.0    7.0      5.0     4.0   2.0   \n",
       "8579     7    2.0      8.0      9.0     7.0    9.0      7.0     3.0   2.0   \n",
       "8580     7    4.0      7.0      8.0     5.0   10.0      4.0     1.0   1.0   \n",
       "8581     7    1.0      6.0      7.0     6.0    8.0      6.0     3.0   1.0   \n",
       "8582     7    6.0      7.0      7.0     6.0    8.0      7.0     3.0   1.0   \n",
       "8583     7    2.0      6.0      8.0     5.0   10.0      6.0     5.0   1.0   \n",
       "8584     7    2.0      8.0     10.0     6.0    9.0      6.0     2.0   1.0   \n",
       "8585     7    1.0      3.0      6.0     4.0    9.0      7.0     3.0   1.0   \n",
       "8586     7    2.0      4.0      6.0     3.0    7.0      7.0     3.0   2.0   \n",
       "8587     7    4.0      4.0      6.0     7.0    9.0      7.0     3.0   1.0   \n",
       "8588     7    1.0      6.0      5.0     5.0   10.0      7.0     2.0   1.0   \n",
       "8589     7    3.0      4.0      5.0     3.0    6.0      6.0     2.0   1.0   \n",
       "8590     7    5.0      6.0      4.0     4.0   10.0      6.0     3.0   1.0   \n",
       "8591     7    4.0      5.0      7.0     6.0    8.0      6.0     3.0   1.0   \n",
       "8592     7    5.0      8.0      8.0     6.0    9.0      7.0     3.0   1.0   \n",
       "8593     7    2.0      6.0      7.0     5.0    7.0      7.0     4.0   2.0   \n",
       "\n",
       "      agea  CH  CZ  DE  ES  NO  SE  \n",
       "0     60.0   1   0   0   0   0   0  \n",
       "1     59.0   1   0   0   0   0   0  \n",
       "2     24.0   1   0   0   0   0   0  \n",
       "3     64.0   1   0   0   0   0   0  \n",
       "4     55.0   1   0   0   0   0   0  \n",
       "6     76.0   1   0   0   0   0   0  \n",
       "7     30.0   1   0   0   0   0   0  \n",
       "8     84.0   1   0   0   0   0   0  \n",
       "9     62.0   1   0   0   0   0   0  \n",
       "10    33.0   1   0   0   0   0   0  \n",
       "11    40.0   1   0   0   0   0   0  \n",
       "12    69.0   1   0   0   0   0   0  \n",
       "13    59.0   1   0   0   0   0   0  \n",
       "14    32.0   1   0   0   0   0   0  \n",
       "15    70.0   1   0   0   0   0   0  \n",
       "16    61.0   1   0   0   0   0   0  \n",
       "17    30.0   1   0   0   0   0   0  \n",
       "18    21.0   1   0   0   0   0   0  \n",
       "19    36.0   1   0   0   0   0   0  \n",
       "20    51.0   1   0   0   0   0   0  \n",
       "21    25.0   1   0   0   0   0   0  \n",
       "22    62.0   1   0   0   0   0   0  \n",
       "23    20.0   1   0   0   0   0   0  \n",
       "24    22.0   1   0   0   0   0   0  \n",
       "25    32.0   1   0   0   0   0   0  \n",
       "26    35.0   1   0   0   0   0   0  \n",
       "27    26.0   1   0   0   0   0   0  \n",
       "28    35.0   1   0   0   0   0   0  \n",
       "29    54.0   1   0   0   0   0   0  \n",
       "30    38.0   1   0   0   0   0   0  \n",
       "...    ...  ..  ..  ..  ..  ..  ..  \n",
       "8564  17.0   0   0   0   0   0   1  \n",
       "8565  17.0   0   0   0   0   0   1  \n",
       "8566  17.0   0   0   0   0   0   1  \n",
       "8567  17.0   0   0   0   0   0   1  \n",
       "8568  17.0   0   0   0   0   0   1  \n",
       "8569  17.0   0   0   0   0   0   1  \n",
       "8570  16.0   0   0   0   0   0   1  \n",
       "8571  16.0   0   0   0   0   0   1  \n",
       "8572  16.0   0   0   0   0   0   1  \n",
       "8573  16.0   0   0   0   0   0   1  \n",
       "8574  16.0   0   0   0   0   0   1  \n",
       "8575  16.0   0   0   0   0   0   1  \n",
       "8576  16.0   0   0   0   0   0   1  \n",
       "8577  16.0   0   0   0   0   0   1  \n",
       "8578  16.0   0   0   0   0   0   1  \n",
       "8579  16.0   0   0   0   0   0   1  \n",
       "8580  16.0   0   0   0   0   0   1  \n",
       "8581  16.0   0   0   0   0   0   1  \n",
       "8582  16.0   0   0   0   0   0   1  \n",
       "8583  16.0   0   0   0   0   0   1  \n",
       "8584  16.0   0   0   0   0   0   1  \n",
       "8585  16.0   0   0   0   0   0   1  \n",
       "8586  15.0   0   0   0   0   0   1  \n",
       "8587  15.0   0   0   0   0   0   1  \n",
       "8588  15.0   0   0   0   0   0   1  \n",
       "8589  18.0   0   0   0   0   0   1  \n",
       "8590  15.0   0   0   0   0   0   1  \n",
       "8591  44.0   0   0   0   0   0   1  \n",
       "8592  15.0   0   0   0   0   0   1  \n",
       "8593  15.0   0   0   0   0   0   1  \n",
       "\n",
       "[8147 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04023458810692853\n",
      "Percent Type II errors: 0.15370976541189307\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.07975460122699386\n",
      "Percent Type II errors: 0.18036809815950922\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'learning_rate': .2,\n",
    "          'subsample': .85,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4213</td>\n",
       "      <td>295</td>\n",
       "      <td>4508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1127</td>\n",
       "      <td>1697</td>\n",
       "      <td>2824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>5340</td>\n",
       "      <td>1992</td>\n",
       "      <td>7332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0.0   1.0   All\n",
       "partner                  \n",
       "0.0      4213   295  4508\n",
       "1.0      1127  1697  2824\n",
       "All      5340  1992  7332"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22e0f9e1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new features\n",
    "\n",
    "Applying more overfitting-prevention strategies like subsampling\n",
    "\n",
    "More iterations\n",
    "\n",
    "Trying a different loss function\n",
    "\n",
    "Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.95      0.86      4508\n",
      "        1.0       0.87      0.58      0.70      2824\n",
      "\n",
      "avg / total       0.82      0.81      0.80      7332\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.93      0.81       505\n",
      "        1.0       0.79      0.40      0.53       310\n",
      "\n",
      "avg / total       0.74      0.73      0.71       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Preprocessing(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "#     ('preprocessing', Preprocessing()),\n",
    "    ('dimensions', PCA()),\n",
    "    #('dummy_vars', ce.OneHotEncoder(cols=['cntry'])),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(classification_report(y_train, pipeline.predict(X_train)))\n",
    "print(classification_report(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed: 13.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('dimensions', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'classifier__n_estimators': [150, 200, 250, 500, 1000], 'classifier__max_depth': [2, 4], 'classifier__learning_rate': [0.99, 0.3, 0.1], 'classifier__subsample': [1.0, 0.3], 'classifier__loss': ['deviance', 'exponential']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch = GridSearchCV(pipeline, {\n",
    "    #'dimensions': [PCA(), Pipeline([])],\n",
    "    'classifier__n_estimators': [150, 200, 250, 500, 1000],\n",
    "    'classifier__max_depth': [2, 4],\n",
    "    'classifier__learning_rate': [0.99, 0.3, 0.1],\n",
    "    'classifier__subsample': [1.0, 0.3],\n",
    "    'classifier__loss': ['deviance', 'exponential']\n",
    "}, scoring='f1', cv=5, refit=True, n_jobs=4, verbose=1)\n",
    "\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='exponential', max_depth=2,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=150,\n",
      "              presort='auto', random_state=None, subsample=0.3, verbose=0,\n",
      "              warm_start=False),\n",
      " 'dimensions': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(gridsearch.best_estimator_.named_steps)\n",
    "df = pd.DataFrame(gridsearch.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_classifier__learning_rate</th>\n",
       "      <th>param_classifier__loss</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>param_classifier__subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.306477</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>0.598230</td>\n",
       "      <td>0.637887</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613277</td>\n",
       "      <td>0.633909</td>\n",
       "      <td>0.602198</td>\n",
       "      <td>0.640977</td>\n",
       "      <td>0.627680</td>\n",
       "      <td>0.634569</td>\n",
       "      <td>0.180283</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.009946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2.038223</td>\n",
       "      <td>0.011430</td>\n",
       "      <td>0.595564</td>\n",
       "      <td>0.665568</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628392</td>\n",
       "      <td>0.667020</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.665962</td>\n",
       "      <td>0.617476</td>\n",
       "      <td>0.660148</td>\n",
       "      <td>0.351011</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.029784</td>\n",
       "      <td>0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>3.775848</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.592823</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631256</td>\n",
       "      <td>0.691127</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.697553</td>\n",
       "      <td>0.629490</td>\n",
       "      <td>0.692527</td>\n",
       "      <td>0.167721</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0.004038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.147654</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>0.591602</td>\n",
       "      <td>0.651120</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623950</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.651547</td>\n",
       "      <td>0.629559</td>\n",
       "      <td>0.658911</td>\n",
       "      <td>0.036011</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.006829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.257747</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>0.651255</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628030</td>\n",
       "      <td>0.643715</td>\n",
       "      <td>0.617801</td>\n",
       "      <td>0.655117</td>\n",
       "      <td>0.642292</td>\n",
       "      <td>0.654011</td>\n",
       "      <td>0.154658</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.006220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.586757</td>\n",
       "      <td>0.698045</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607393</td>\n",
       "      <td>0.706944</td>\n",
       "      <td>0.556034</td>\n",
       "      <td>0.703732</td>\n",
       "      <td>0.633588</td>\n",
       "      <td>0.695872</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.006988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.569978</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.585527</td>\n",
       "      <td>0.657202</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629474</td>\n",
       "      <td>0.650054</td>\n",
       "      <td>0.590604</td>\n",
       "      <td>0.654226</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.653591</td>\n",
       "      <td>0.064842</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.040940</td>\n",
       "      <td>0.007067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.113563</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.583024</td>\n",
       "      <td>0.688767</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610951</td>\n",
       "      <td>0.682902</td>\n",
       "      <td>0.607880</td>\n",
       "      <td>0.697849</td>\n",
       "      <td>0.640125</td>\n",
       "      <td>0.688990</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>0.005196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.431207</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>0.582142</td>\n",
       "      <td>0.703478</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632860</td>\n",
       "      <td>0.707490</td>\n",
       "      <td>0.579208</td>\n",
       "      <td>0.711089</td>\n",
       "      <td>0.615949</td>\n",
       "      <td>0.702256</td>\n",
       "      <td>0.053790</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.044202</td>\n",
       "      <td>0.008188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3.454996</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>0.581684</td>\n",
       "      <td>0.763276</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636917</td>\n",
       "      <td>0.766522</td>\n",
       "      <td>0.593361</td>\n",
       "      <td>0.759079</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.760742</td>\n",
       "      <td>0.611548</td>\n",
       "      <td>0.005358</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>0.003804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.867971</td>\n",
       "      <td>0.015441</td>\n",
       "      <td>0.581288</td>\n",
       "      <td>0.672992</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616977</td>\n",
       "      <td>0.676502</td>\n",
       "      <td>0.588745</td>\n",
       "      <td>0.679265</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>0.665077</td>\n",
       "      <td>0.071269</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.051539</td>\n",
       "      <td>0.007428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.413160</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.572890</td>\n",
       "      <td>0.779577</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612403</td>\n",
       "      <td>0.782757</td>\n",
       "      <td>0.572086</td>\n",
       "      <td>0.775731</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.781227</td>\n",
       "      <td>0.054494</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.036937</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.029791</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.570866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.470019</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.062067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3.439386</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.569714</td>\n",
       "      <td>0.713729</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.718670</td>\n",
       "      <td>0.599291</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.651449</td>\n",
       "      <td>0.711786</td>\n",
       "      <td>0.438524</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.084941</td>\n",
       "      <td>0.004478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.082080</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.568728</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630846</td>\n",
       "      <td>0.675225</td>\n",
       "      <td>0.590336</td>\n",
       "      <td>0.679419</td>\n",
       "      <td>0.624426</td>\n",
       "      <td>0.667351</td>\n",
       "      <td>0.101347</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.081733</td>\n",
       "      <td>0.006436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.771158</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.565554</td>\n",
       "      <td>0.804003</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606975</td>\n",
       "      <td>0.801983</td>\n",
       "      <td>0.590420</td>\n",
       "      <td>0.809175</td>\n",
       "      <td>0.592322</td>\n",
       "      <td>0.823780</td>\n",
       "      <td>0.130036</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.046614</td>\n",
       "      <td>0.012805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.247482</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.563424</td>\n",
       "      <td>0.749034</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605364</td>\n",
       "      <td>0.745088</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.737543</td>\n",
       "      <td>0.606586</td>\n",
       "      <td>0.761169</td>\n",
       "      <td>0.265983</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.068522</td>\n",
       "      <td>0.008010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.558230</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.562702</td>\n",
       "      <td>0.652565</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559367</td>\n",
       "      <td>0.650882</td>\n",
       "      <td>0.549306</td>\n",
       "      <td>0.673006</td>\n",
       "      <td>0.585911</td>\n",
       "      <td>0.646327</td>\n",
       "      <td>0.153364</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.010396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.600658</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.559920</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.627920</td>\n",
       "      <td>0.583423</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.622584</td>\n",
       "      <td>0.629825</td>\n",
       "      <td>0.142793</td>\n",
       "      <td>0.009289</td>\n",
       "      <td>0.082849</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.861337</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.559763</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622561</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.203879</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.086986</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.447252</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.558693</td>\n",
       "      <td>0.739230</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578397</td>\n",
       "      <td>0.754853</td>\n",
       "      <td>0.540906</td>\n",
       "      <td>0.733531</td>\n",
       "      <td>0.577849</td>\n",
       "      <td>0.746128</td>\n",
       "      <td>0.053485</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.012502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.991183</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.557052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129161</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.088105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8.556125</td>\n",
       "      <td>0.025669</td>\n",
       "      <td>0.556864</td>\n",
       "      <td>0.773538</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.773227</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>0.780971</td>\n",
       "      <td>0.619796</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>1.748631</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.112351</td>\n",
       "      <td>0.005374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4.367623</td>\n",
       "      <td>0.032085</td>\n",
       "      <td>0.555798</td>\n",
       "      <td>0.742936</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.742755</td>\n",
       "      <td>0.601105</td>\n",
       "      <td>0.737487</td>\n",
       "      <td>0.614085</td>\n",
       "      <td>0.742358</td>\n",
       "      <td>0.624394</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>0.099778</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.379271</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>0.555297</td>\n",
       "      <td>0.693325</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588879</td>\n",
       "      <td>0.693290</td>\n",
       "      <td>0.580256</td>\n",
       "      <td>0.689192</td>\n",
       "      <td>0.595007</td>\n",
       "      <td>0.699748</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.061985</td>\n",
       "      <td>0.003649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>11.505950</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.555201</td>\n",
       "      <td>0.999690</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637868</td>\n",
       "      <td>0.999114</td>\n",
       "      <td>0.576520</td>\n",
       "      <td>0.999336</td>\n",
       "      <td>0.600355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826879</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2.694370</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>0.555015</td>\n",
       "      <td>0.783463</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628337</td>\n",
       "      <td>0.779823</td>\n",
       "      <td>0.591463</td>\n",
       "      <td>0.783988</td>\n",
       "      <td>0.605898</td>\n",
       "      <td>0.784235</td>\n",
       "      <td>0.170507</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.005185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.922315</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.554948</td>\n",
       "      <td>0.646285</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618026</td>\n",
       "      <td>0.643621</td>\n",
       "      <td>0.590348</td>\n",
       "      <td>0.649986</td>\n",
       "      <td>0.617409</td>\n",
       "      <td>0.642348</td>\n",
       "      <td>0.070266</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.092955</td>\n",
       "      <td>0.007516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.929225</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.554732</td>\n",
       "      <td>0.997915</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628253</td>\n",
       "      <td>0.996892</td>\n",
       "      <td>0.555394</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>0.602058</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>0.138906</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.075262</td>\n",
       "      <td>0.000519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>4.010272</td>\n",
       "      <td>0.029479</td>\n",
       "      <td>0.554386</td>\n",
       "      <td>0.718520</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640562</td>\n",
       "      <td>0.711944</td>\n",
       "      <td>0.596950</td>\n",
       "      <td>0.723459</td>\n",
       "      <td>0.626728</td>\n",
       "      <td>0.711749</td>\n",
       "      <td>0.427086</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.116261</td>\n",
       "      <td>0.007969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>19.416262</td>\n",
       "      <td>0.036698</td>\n",
       "      <td>0.511134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515325</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.169430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.514772</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.511067</td>\n",
       "      <td>0.942741</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.940686</td>\n",
       "      <td>0.551117</td>\n",
       "      <td>0.942103</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.945278</td>\n",
       "      <td>0.292136</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.166839</td>\n",
       "      <td>0.002431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>6.115073</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.510963</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641393</td>\n",
       "      <td>0.827049</td>\n",
       "      <td>0.615679</td>\n",
       "      <td>0.811579</td>\n",
       "      <td>0.613124</td>\n",
       "      <td>0.814060</td>\n",
       "      <td>0.398764</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.201992</td>\n",
       "      <td>0.008575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>9.455666</td>\n",
       "      <td>0.024064</td>\n",
       "      <td>0.510914</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581571</td>\n",
       "      <td>0.999557</td>\n",
       "      <td>0.605787</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.267278</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.181610</td>\n",
       "      <td>0.000198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.970337</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.981535</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.980726</td>\n",
       "      <td>0.577215</td>\n",
       "      <td>0.980709</td>\n",
       "      <td>0.603969</td>\n",
       "      <td>0.979153</td>\n",
       "      <td>0.160805</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.166137</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.844108</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.735255</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632411</td>\n",
       "      <td>0.740226</td>\n",
       "      <td>0.568116</td>\n",
       "      <td>0.735554</td>\n",
       "      <td>0.616257</td>\n",
       "      <td>0.731266</td>\n",
       "      <td>0.085226</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.179760</td>\n",
       "      <td>0.005115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.443825</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.508181</td>\n",
       "      <td>0.786614</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632124</td>\n",
       "      <td>0.800102</td>\n",
       "      <td>0.614913</td>\n",
       "      <td>0.786488</td>\n",
       "      <td>0.608095</td>\n",
       "      <td>0.780933</td>\n",
       "      <td>0.122353</td>\n",
       "      <td>0.006290</td>\n",
       "      <td>0.200141</td>\n",
       "      <td>0.008982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.976041</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.507231</td>\n",
       "      <td>0.844620</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623274</td>\n",
       "      <td>0.847607</td>\n",
       "      <td>0.557888</td>\n",
       "      <td>0.840970</td>\n",
       "      <td>0.616341</td>\n",
       "      <td>0.844925</td>\n",
       "      <td>0.397513</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.174576</td>\n",
       "      <td>0.005782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.223351</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>0.506714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.259066</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.151732</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20.098350</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.505920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.367394</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4.031930</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.505706</td>\n",
       "      <td>0.752239</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636079</td>\n",
       "      <td>0.770109</td>\n",
       "      <td>0.605416</td>\n",
       "      <td>0.752034</td>\n",
       "      <td>0.617085</td>\n",
       "      <td>0.751184</td>\n",
       "      <td>0.213664</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.206913</td>\n",
       "      <td>0.011651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.445566</td>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.504287</td>\n",
       "      <td>0.635789</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594266</td>\n",
       "      <td>0.649676</td>\n",
       "      <td>0.420339</td>\n",
       "      <td>0.653196</td>\n",
       "      <td>0.569632</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.441543</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.089204</td>\n",
       "      <td>0.022835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11.569991</td>\n",
       "      <td>0.024866</td>\n",
       "      <td>0.504261</td>\n",
       "      <td>0.915565</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.921764</td>\n",
       "      <td>0.602552</td>\n",
       "      <td>0.910480</td>\n",
       "      <td>0.590865</td>\n",
       "      <td>0.916805</td>\n",
       "      <td>0.426887</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.004003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.924520</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.503241</td>\n",
       "      <td>0.802142</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558909</td>\n",
       "      <td>0.803973</td>\n",
       "      <td>0.551167</td>\n",
       "      <td>0.779808</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.823390</td>\n",
       "      <td>0.155648</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.126381</td>\n",
       "      <td>0.014014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.706342</td>\n",
       "      <td>0.009824</td>\n",
       "      <td>0.503239</td>\n",
       "      <td>0.782660</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594286</td>\n",
       "      <td>0.786718</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.773673</td>\n",
       "      <td>0.602643</td>\n",
       "      <td>0.781809</td>\n",
       "      <td>0.063236</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.168647</td>\n",
       "      <td>0.005462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.434816</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.502720</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624754</td>\n",
       "      <td>0.979941</td>\n",
       "      <td>0.570108</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.974672</td>\n",
       "      <td>0.405829</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.182790</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.916987</td>\n",
       "      <td>0.023061</td>\n",
       "      <td>0.501670</td>\n",
       "      <td>0.904699</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570652</td>\n",
       "      <td>0.892599</td>\n",
       "      <td>0.538389</td>\n",
       "      <td>0.900695</td>\n",
       "      <td>0.571170</td>\n",
       "      <td>0.917040</td>\n",
       "      <td>0.060716</td>\n",
       "      <td>0.006049</td>\n",
       "      <td>0.124383</td>\n",
       "      <td>0.011002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.036337</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>0.500977</td>\n",
       "      <td>0.800855</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563526</td>\n",
       "      <td>0.796840</td>\n",
       "      <td>0.546694</td>\n",
       "      <td>0.798392</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.803349</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.110913</td>\n",
       "      <td>0.003440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.774001</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.496820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.603993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.188426</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.188643</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.112145</td>\n",
       "      <td>0.021055</td>\n",
       "      <td>0.496216</td>\n",
       "      <td>0.951884</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628116</td>\n",
       "      <td>0.954179</td>\n",
       "      <td>0.562072</td>\n",
       "      <td>0.953520</td>\n",
       "      <td>0.596206</td>\n",
       "      <td>0.948463</td>\n",
       "      <td>0.382207</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>0.190798</td>\n",
       "      <td>0.004212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.175721</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.496016</td>\n",
       "      <td>0.558070</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496768</td>\n",
       "      <td>0.547872</td>\n",
       "      <td>0.524224</td>\n",
       "      <td>0.574535</td>\n",
       "      <td>0.533437</td>\n",
       "      <td>0.596358</td>\n",
       "      <td>0.099186</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.055838</td>\n",
       "      <td>0.065750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.209851</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>0.492642</td>\n",
       "      <td>0.973783</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605948</td>\n",
       "      <td>0.975774</td>\n",
       "      <td>0.550813</td>\n",
       "      <td>0.973874</td>\n",
       "      <td>0.597134</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.176617</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.517689</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.489983</td>\n",
       "      <td>0.876008</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604915</td>\n",
       "      <td>0.876342</td>\n",
       "      <td>0.545821</td>\n",
       "      <td>0.877741</td>\n",
       "      <td>0.601784</td>\n",
       "      <td>0.875294</td>\n",
       "      <td>0.043054</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.183703</td>\n",
       "      <td>0.001047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.397787</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.487933</td>\n",
       "      <td>0.809361</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594852</td>\n",
       "      <td>0.809901</td>\n",
       "      <td>0.551464</td>\n",
       "      <td>0.809569</td>\n",
       "      <td>0.608624</td>\n",
       "      <td>0.813282</td>\n",
       "      <td>0.289511</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.189538</td>\n",
       "      <td>0.002545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.258952</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.996590</td>\n",
       "      <td>0.99</td>\n",
       "      <td>exponential</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.998008</td>\n",
       "      <td>0.534009</td>\n",
       "      <td>0.995126</td>\n",
       "      <td>0.569355</td>\n",
       "      <td>0.998008</td>\n",
       "      <td>0.387531</td>\n",
       "      <td>0.015306</td>\n",
       "      <td>0.164361</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10.068970</td>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.484096</td>\n",
       "      <td>0.803573</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.3, 'classifier...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548214</td>\n",
       "      <td>0.803661</td>\n",
       "      <td>0.423032</td>\n",
       "      <td>0.778517</td>\n",
       "      <td>0.590835</td>\n",
       "      <td>0.817347</td>\n",
       "      <td>0.524774</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.105641</td>\n",
       "      <td>0.013332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.263684</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.469196</td>\n",
       "      <td>0.847281</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590781</td>\n",
       "      <td>0.848113</td>\n",
       "      <td>0.485169</td>\n",
       "      <td>0.848370</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.847909</td>\n",
       "      <td>0.102982</td>\n",
       "      <td>0.008494</td>\n",
       "      <td>0.186186</td>\n",
       "      <td>0.001082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.011603</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>0.451410</td>\n",
       "      <td>0.467082</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548164</td>\n",
       "      <td>0.551924</td>\n",
       "      <td>0.390139</td>\n",
       "      <td>0.479384</td>\n",
       "      <td>0.455639</td>\n",
       "      <td>0.501325</td>\n",
       "      <td>0.382969</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.076922</td>\n",
       "      <td>0.090245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.000131</td>\n",
       "      <td>0.048931</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.510440</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474453</td>\n",
       "      <td>0.479263</td>\n",
       "      <td>0.500666</td>\n",
       "      <td>0.534212</td>\n",
       "      <td>0.500556</td>\n",
       "      <td>0.523317</td>\n",
       "      <td>0.658981</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.070134</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.187584</td>\n",
       "      <td>0.017847</td>\n",
       "      <td>0.411934</td>\n",
       "      <td>0.481389</td>\n",
       "      <td>0.99</td>\n",
       "      <td>deviance</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'classifier__learning_rate': 0.99, 'classifie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474679</td>\n",
       "      <td>0.456702</td>\n",
       "      <td>0.461843</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.347758</td>\n",
       "      <td>0.326718</td>\n",
       "      <td>0.279601</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.066043</td>\n",
       "      <td>0.092846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "101       1.306477         0.008623         0.598230          0.637887   \n",
       "83        2.038223         0.011430         0.595564          0.665568   \n",
       "107       3.775848         0.015641         0.592823          0.692250   \n",
       "103       1.147654         0.006417         0.591602          0.651120   \n",
       "81        1.257747         0.011229         0.589109          0.651255   \n",
       "41        0.895784         0.008824         0.586757          0.698045   \n",
       "105       1.569978         0.011631         0.585527          0.657202   \n",
       "63        1.113563         0.006818         0.583024          0.688767   \n",
       "65        1.431207         0.008021         0.582142          0.703478   \n",
       "115       3.454996         0.017245         0.581684          0.763276   \n",
       "85        1.867971         0.015441         0.581288          0.672992   \n",
       "20        1.413160         0.009425         0.572890          0.779577   \n",
       "36       10.029791         0.022460         0.570866          1.000000   \n",
       "87        3.439386         0.014556         0.569714          0.713729   \n",
       "61        1.082080         0.009625         0.568728          0.675424   \n",
       "49        5.771158         0.022259         0.565554          0.804003   \n",
       "67        3.247482         0.012834         0.563424          0.749034   \n",
       "5         1.558230         0.006818         0.562702          0.652565   \n",
       "100       1.600658         0.010629         0.559920          0.634101   \n",
       "34        4.861337         0.012634         0.559763          0.999956   \n",
       "31        1.447252         0.009224         0.558693          0.739230   \n",
       "14        4.991183         0.011631         0.557052          1.000000   \n",
       "89        8.556125         0.025669         0.556864          0.773538   \n",
       "113       4.367623         0.032085         0.555798          0.742936   \n",
       "25        1.379271         0.007419         0.555297          0.693325   \n",
       "79       11.505950         0.053944         0.555201          0.999690   \n",
       "93        2.694370         0.019051         0.555015          0.783463   \n",
       "102       1.922315         0.008624         0.554948          0.646285   \n",
       "32        3.929225         0.009827         0.554732          0.997915   \n",
       "111       4.010272         0.029479         0.554386          0.718520   \n",
       "..             ...              ...              ...               ...   \n",
       "58       19.416262         0.036698         0.511134          1.000000   \n",
       "48        9.514772         0.024265         0.511067          0.942741   \n",
       "94        6.115073         0.012634         0.510963          0.814450   \n",
       "56        9.455666         0.024064         0.510914          0.999779   \n",
       "30        2.970337         0.007821         0.510488          0.981535   \n",
       "42        1.844108         0.007219         0.509336          0.735255   \n",
       "92        4.443825         0.016244         0.508181          0.786614   \n",
       "46        4.976041         0.012834         0.507231          0.844620   \n",
       "8        10.223351         0.022861         0.506714          1.000000   \n",
       "78       20.098350         0.038703         0.505920          1.000000   \n",
       "90        4.031930         0.024265         0.505706          0.752239   \n",
       "9         6.445566         0.021173         0.504287          0.635789   \n",
       "96       11.569991         0.024866         0.504261          0.915565   \n",
       "51        1.924520         0.011832         0.503241          0.802142   \n",
       "71        1.706342         0.009824         0.503239          0.782660   \n",
       "54        5.434816         0.012033         0.502720          0.976351   \n",
       "37        4.916987         0.023061         0.501670          0.904699   \n",
       "29        6.036337         0.029229         0.500977          0.800855   \n",
       "16        9.774001         0.019770         0.496820          1.000000   \n",
       "52        4.112145         0.021055         0.496216          0.951884   \n",
       "13        2.175721         0.013637         0.496016          0.558070   \n",
       "6         5.209851         0.013837         0.492642          0.973783   \n",
       "4         2.517689         0.011631         0.489983          0.876008   \n",
       "0         1.397787         0.004328         0.487933          0.809361   \n",
       "39       10.258952         0.046725         0.484277          0.996590   \n",
       "59       10.068970         0.033089         0.484096          0.803573   \n",
       "2         2.263684         0.011230         0.469196          0.847281   \n",
       "17        6.011603         0.022861         0.451410          0.467082   \n",
       "19       12.000131         0.048931         0.444043          0.510440   \n",
       "15        3.187584         0.017847         0.411934          0.481389   \n",
       "\n",
       "    param_classifier__learning_rate param_classifier__loss  \\\n",
       "101                             0.1            exponential   \n",
       "83                              0.1               deviance   \n",
       "107                             0.1            exponential   \n",
       "103                             0.1            exponential   \n",
       "81                              0.1               deviance   \n",
       "41                              0.3               deviance   \n",
       "105                             0.1            exponential   \n",
       "63                              0.3            exponential   \n",
       "65                              0.3            exponential   \n",
       "115                             0.1            exponential   \n",
       "85                              0.1               deviance   \n",
       "20                             0.99            exponential   \n",
       "36                             0.99            exponential   \n",
       "87                              0.1               deviance   \n",
       "61                              0.3            exponential   \n",
       "49                              0.3               deviance   \n",
       "67                              0.3            exponential   \n",
       "5                              0.99               deviance   \n",
       "100                             0.1            exponential   \n",
       "34                             0.99            exponential   \n",
       "31                             0.99            exponential   \n",
       "14                             0.99               deviance   \n",
       "89                              0.1               deviance   \n",
       "113                             0.1            exponential   \n",
       "25                             0.99            exponential   \n",
       "79                              0.3            exponential   \n",
       "93                              0.1               deviance   \n",
       "102                             0.1            exponential   \n",
       "32                             0.99            exponential   \n",
       "111                             0.1            exponential   \n",
       "..                              ...                    ...   \n",
       "58                              0.3               deviance   \n",
       "48                              0.3               deviance   \n",
       "94                              0.1               deviance   \n",
       "56                              0.3               deviance   \n",
       "30                             0.99            exponential   \n",
       "42                              0.3               deviance   \n",
       "92                              0.1               deviance   \n",
       "46                              0.3               deviance   \n",
       "8                              0.99               deviance   \n",
       "78                              0.3            exponential   \n",
       "90                              0.1               deviance   \n",
       "9                              0.99               deviance   \n",
       "96                              0.1               deviance   \n",
       "51                              0.3               deviance   \n",
       "71                              0.3            exponential   \n",
       "54                              0.3               deviance   \n",
       "37                             0.99            exponential   \n",
       "29                             0.99            exponential   \n",
       "16                             0.99               deviance   \n",
       "52                              0.3               deviance   \n",
       "13                             0.99               deviance   \n",
       "6                              0.99               deviance   \n",
       "4                              0.99               deviance   \n",
       "0                              0.99               deviance   \n",
       "39                             0.99            exponential   \n",
       "59                              0.3               deviance   \n",
       "2                              0.99               deviance   \n",
       "17                             0.99               deviance   \n",
       "19                             0.99               deviance   \n",
       "15                             0.99               deviance   \n",
       "\n",
       "    param_classifier__max_depth param_classifier__n_estimators  \\\n",
       "101                           2                            150   \n",
       "83                            2                            200   \n",
       "107                           2                            500   \n",
       "103                           2                            200   \n",
       "81                            2                            150   \n",
       "41                            2                            150   \n",
       "105                           2                            250   \n",
       "63                            2                            200   \n",
       "65                            2                            250   \n",
       "115                           4                            250   \n",
       "85                            2                            250   \n",
       "20                            2                            150   \n",
       "36                            4                            500   \n",
       "87                            2                            500   \n",
       "61                            2                            150   \n",
       "49                            2                           1000   \n",
       "67                            2                            500   \n",
       "5                             2                            250   \n",
       "100                           2                            150   \n",
       "34                            4                            250   \n",
       "31                            4                            150   \n",
       "14                            4                            250   \n",
       "89                            2                           1000   \n",
       "113                           4                            200   \n",
       "25                            2                            250   \n",
       "79                            4                           1000   \n",
       "93                            4                            200   \n",
       "102                           2                            200   \n",
       "32                            4                            200   \n",
       "111                           4                            150   \n",
       "..                          ...                            ...   \n",
       "58                            4                           1000   \n",
       "48                            2                           1000   \n",
       "94                            4                            250   \n",
       "56                            4                            500   \n",
       "30                            4                            150   \n",
       "42                            2                            200   \n",
       "92                            4                            200   \n",
       "46                            2                            500   \n",
       "8                             2                           1000   \n",
       "78                            4                           1000   \n",
       "90                            4                            150   \n",
       "9                             2                           1000   \n",
       "96                            4                            500   \n",
       "51                            4                            150   \n",
       "71                            4                            150   \n",
       "54                            4                            250   \n",
       "37                            4                            500   \n",
       "29                            2                           1000   \n",
       "16                            4                            500   \n",
       "52                            4                            200   \n",
       "13                            4                            200   \n",
       "6                             2                            500   \n",
       "4                             2                            250   \n",
       "0                             2                            150   \n",
       "39                            4                           1000   \n",
       "59                            4                           1000   \n",
       "2                             2                            200   \n",
       "17                            4                            500   \n",
       "19                            4                           1000   \n",
       "15                            4                            250   \n",
       "\n",
       "    param_classifier__subsample  \\\n",
       "101                         0.3   \n",
       "83                          0.3   \n",
       "107                         0.3   \n",
       "103                         0.3   \n",
       "81                          0.3   \n",
       "41                          0.3   \n",
       "105                         0.3   \n",
       "63                          0.3   \n",
       "65                          0.3   \n",
       "115                         0.3   \n",
       "85                          0.3   \n",
       "20                            1   \n",
       "36                            1   \n",
       "87                          0.3   \n",
       "61                          0.3   \n",
       "49                          0.3   \n",
       "67                          0.3   \n",
       "5                           0.3   \n",
       "100                           1   \n",
       "34                            1   \n",
       "31                          0.3   \n",
       "14                            1   \n",
       "89                          0.3   \n",
       "113                         0.3   \n",
       "25                          0.3   \n",
       "79                          0.3   \n",
       "93                          0.3   \n",
       "102                           1   \n",
       "32                            1   \n",
       "111                         0.3   \n",
       "..                          ...   \n",
       "58                            1   \n",
       "48                            1   \n",
       "94                            1   \n",
       "56                            1   \n",
       "30                            1   \n",
       "42                            1   \n",
       "92                            1   \n",
       "46                            1   \n",
       "8                             1   \n",
       "78                            1   \n",
       "90                            1   \n",
       "9                           0.3   \n",
       "96                            1   \n",
       "51                          0.3   \n",
       "71                          0.3   \n",
       "54                            1   \n",
       "37                          0.3   \n",
       "29                          0.3   \n",
       "16                            1   \n",
       "52                            1   \n",
       "13                          0.3   \n",
       "6                             1   \n",
       "4                             1   \n",
       "0                             1   \n",
       "39                          0.3   \n",
       "59                          0.3   \n",
       "2                             1   \n",
       "17                          0.3   \n",
       "19                          0.3   \n",
       "15                          0.3   \n",
       "\n",
       "                                                params       ...         \\\n",
       "101  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "83   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "107  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "103  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "81   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "41   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "105  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "63   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "65   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "115  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "85   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "20   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "36   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "87   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "61   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "49   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "67   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "5    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "100  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "34   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "31   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "14   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "89   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "113  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "25   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "79   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "93   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "102  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "32   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "111  {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "..                                                 ...       ...          \n",
       "58   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "48   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "94   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "56   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "30   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "42   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "92   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "46   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "8    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "78   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "90   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "9    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "96   {'classifier__learning_rate': 0.1, 'classifier...       ...          \n",
       "51   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "71   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "54   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "37   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "29   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "16   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "52   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "13   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "6    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "4    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "0    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "39   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "59   {'classifier__learning_rate': 0.3, 'classifier...       ...          \n",
       "2    {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "17   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "19   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "15   {'classifier__learning_rate': 0.99, 'classifie...       ...          \n",
       "\n",
       "     split2_test_score  split2_train_score  split3_test_score  \\\n",
       "101           0.613277            0.633909           0.602198   \n",
       "83            0.628392            0.667020           0.600000   \n",
       "107           0.631256            0.691127           0.603774   \n",
       "103           0.623950            0.645900           0.603376   \n",
       "81            0.628030            0.643715           0.617801   \n",
       "41            0.607393            0.706944           0.556034   \n",
       "105           0.629474            0.650054           0.590604   \n",
       "63            0.610951            0.682902           0.607880   \n",
       "65            0.632860            0.707490           0.579208   \n",
       "115           0.636917            0.766522           0.593361   \n",
       "85            0.616977            0.676502           0.588745   \n",
       "20            0.612403            0.782757           0.572086   \n",
       "36            0.628193            1.000000           0.584759   \n",
       "87            0.612613            0.718670           0.599291   \n",
       "61            0.630846            0.675225           0.590336   \n",
       "49            0.606975            0.801983           0.590420   \n",
       "67            0.605364            0.745088           0.596078   \n",
       "5             0.559367            0.650882           0.549306   \n",
       "100           0.620690            0.627920           0.583423   \n",
       "34            0.637627            1.000000           0.572089   \n",
       "31            0.578397            0.754853           0.540906   \n",
       "14            0.623070            1.000000           0.590258   \n",
       "89            0.624141            0.773227           0.619748   \n",
       "113           0.621951            0.742755           0.601105   \n",
       "25            0.588879            0.693290           0.580256   \n",
       "79            0.637868            0.999114           0.576520   \n",
       "93            0.628337            0.779823           0.591463   \n",
       "102           0.618026            0.643621           0.590348   \n",
       "32            0.628253            0.996892           0.555394   \n",
       "111           0.640562            0.711944           0.596950   \n",
       "..                 ...                 ...                ...   \n",
       "58            0.632085            1.000000           0.576327   \n",
       "48            0.626719            0.940686           0.551117   \n",
       "94            0.641393            0.827049           0.615679   \n",
       "56            0.638916            1.000000           0.581571   \n",
       "30            0.619403            0.980726           0.577215   \n",
       "42            0.632411            0.740226           0.568116   \n",
       "92            0.632124            0.800102           0.614913   \n",
       "46            0.623274            0.847607           0.557888   \n",
       "8             0.600561            1.000000           0.554455   \n",
       "78            0.615236            1.000000           0.572285   \n",
       "90            0.636079            0.770109           0.605416   \n",
       "9             0.594266            0.649676           0.420339   \n",
       "96            0.634146            0.921764           0.602552   \n",
       "51            0.558909            0.803973           0.551167   \n",
       "71            0.594286            0.786718           0.586957   \n",
       "54            0.624754            0.979941           0.570108   \n",
       "37            0.570652            0.892599           0.538389   \n",
       "29            0.563526            0.796840           0.546694   \n",
       "16            0.623616            1.000000           0.562586   \n",
       "52            0.628116            0.954179           0.562072   \n",
       "13            0.496768            0.547872           0.524224   \n",
       "6             0.605948            0.975774           0.550813   \n",
       "4             0.604915            0.876342           0.545821   \n",
       "0             0.594852            0.809901           0.551464   \n",
       "39            0.595633            0.998008           0.534009   \n",
       "59            0.548214            0.803661           0.423032   \n",
       "2             0.590781            0.848113           0.485169   \n",
       "17            0.548164            0.551924           0.390139   \n",
       "19            0.474453            0.479263           0.500666   \n",
       "15            0.474679            0.456702           0.461843   \n",
       "\n",
       "     split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "101            0.640977           0.627680            0.634569      0.180283   \n",
       "83             0.665962           0.617476            0.660148      0.351011   \n",
       "107            0.697553           0.629490            0.692527      0.167721   \n",
       "103            0.651547           0.629559            0.658911      0.036011   \n",
       "81             0.655117           0.642292            0.654011      0.154658   \n",
       "41             0.703732           0.633588            0.695872      0.032599   \n",
       "105            0.654226           0.606250            0.653591      0.064842   \n",
       "63             0.697849           0.640125            0.688990      0.039279   \n",
       "65             0.711089           0.615949            0.702256      0.053790   \n",
       "115            0.759079           0.613636            0.760742      0.611548   \n",
       "85             0.679265           0.634483            0.665077      0.071269   \n",
       "20             0.775731           0.592593            0.781227      0.054494   \n",
       "36             1.000000           0.615120            1.000000      0.470019   \n",
       "87             0.716049           0.651449            0.711786      0.438524   \n",
       "61             0.679419           0.624426            0.667351      0.101347   \n",
       "49             0.809175           0.592322            0.823780      0.130036   \n",
       "67             0.737543           0.606586            0.761169      0.265983   \n",
       "5              0.673006           0.585911            0.646327      0.153364   \n",
       "100            0.641791           0.622584            0.629825      0.142793   \n",
       "34             1.000000           0.622561            0.999779      0.203879   \n",
       "31             0.733531           0.577849            0.746128      0.053485   \n",
       "14             1.000000           0.613200            1.000000      0.129161   \n",
       "89             0.780971           0.619796            0.772455      1.748631   \n",
       "113            0.737487           0.614085            0.742358      0.624394   \n",
       "25             0.689192           0.595007            0.699748      0.034830   \n",
       "79             0.999336           0.600355            1.000000      0.826879   \n",
       "93             0.783988           0.605898            0.784235      0.170507   \n",
       "102            0.649986           0.617409            0.642348      0.070266   \n",
       "32             0.998226           0.602058            0.998227      0.138906   \n",
       "111            0.723459           0.626728            0.711749      0.427086   \n",
       "..                  ...                ...                 ...           ...   \n",
       "58             1.000000           0.597473            1.000000      0.515325   \n",
       "48             0.942103           0.619048            0.945278      0.292136   \n",
       "94             0.811579           0.613124            0.814060      0.398764   \n",
       "56             0.999557           0.605787            0.999779      0.267278   \n",
       "30             0.980709           0.603969            0.979153      0.160805   \n",
       "42             0.735554           0.616257            0.731266      0.085226   \n",
       "92             0.786488           0.608095            0.780933      0.122353   \n",
       "46             0.840970           0.616341            0.844925      0.397513   \n",
       "8              1.000000           0.605028            1.000000      0.259066   \n",
       "78             1.000000           0.606884            1.000000      0.367394   \n",
       "90             0.752034           0.617085            0.751184      0.213664   \n",
       "9              0.653196           0.569632            0.656000      0.441543   \n",
       "96             0.910480           0.590865            0.916805      0.426887   \n",
       "51             0.779808           0.598930            0.823390      0.155648   \n",
       "71             0.773673           0.602643            0.781809      0.063236   \n",
       "54             0.977216           0.600000            0.974672      0.405829   \n",
       "37             0.900695           0.571170            0.917040      0.060716   \n",
       "29             0.798392           0.556962            0.803349      0.321150   \n",
       "16             1.000000           0.603993            1.000000      0.188426   \n",
       "52             0.953520           0.596206            0.948463      0.382207   \n",
       "13             0.574535           0.533437            0.596358      0.099186   \n",
       "6              0.973874           0.597134            0.971429      0.235507   \n",
       "4              0.877741           0.601784            0.875294      0.043054   \n",
       "0              0.809569           0.608624            0.813282      0.289511   \n",
       "39             0.995126           0.569355            0.998008      0.387531   \n",
       "59             0.778517           0.590835            0.817347      0.524774   \n",
       "2              0.848370           0.590476            0.847909      0.102982   \n",
       "17             0.479384           0.455639            0.501325      0.382969   \n",
       "19             0.534212           0.500556            0.523317      0.658981   \n",
       "15             0.597938           0.347758            0.326718      0.279601   \n",
       "\n",
       "     std_score_time  std_test_score  std_train_score  \n",
       "101        0.005214        0.023105         0.009946  \n",
       "83         0.002338        0.029784         0.005038  \n",
       "107        0.002251        0.042623         0.004038  \n",
       "103        0.000490        0.042713         0.006829  \n",
       "81         0.004887        0.073456         0.006220  \n",
       "41         0.003665        0.029441         0.006988  \n",
       "105        0.005253        0.040940         0.007067  \n",
       "63         0.000400        0.053626         0.005196  \n",
       "65         0.000634        0.044202         0.008188  \n",
       "115        0.005358        0.051377         0.003804  \n",
       "85         0.003836        0.051539         0.007428  \n",
       "20         0.006102        0.036937         0.002710  \n",
       "36         0.003080        0.062067         0.000000  \n",
       "87         0.001688        0.084941         0.004478  \n",
       "61         0.004467        0.081733         0.006436  \n",
       "49         0.000983        0.046614         0.012805  \n",
       "67         0.000750        0.068522         0.008010  \n",
       "5          0.004129        0.012270         0.010396  \n",
       "100        0.009289        0.082849         0.008186  \n",
       "34         0.003209        0.086986         0.000088  \n",
       "31         0.002934        0.020491         0.012502  \n",
       "14         0.000491        0.088105         0.000000  \n",
       "89         0.004187        0.112351         0.005374  \n",
       "113        0.014208        0.099778         0.003145  \n",
       "25         0.000491        0.061985         0.003649  \n",
       "79         0.009975        0.087705         0.000386  \n",
       "93         0.013088        0.091900         0.005185  \n",
       "102        0.002423        0.092955         0.007516  \n",
       "32         0.000401        0.075262         0.000519  \n",
       "111        0.012661        0.116261         0.007969  \n",
       "..              ...             ...              ...  \n",
       "58         0.002160        0.169430         0.000000  \n",
       "48         0.002568        0.166839         0.002431  \n",
       "94         0.002251        0.201992         0.008575  \n",
       "56         0.005230        0.181610         0.000198  \n",
       "30         0.001169        0.166137         0.001775  \n",
       "42         0.000750        0.179760         0.005115  \n",
       "92         0.006290        0.200141         0.008982  \n",
       "46         0.001169        0.174576         0.005782  \n",
       "8          0.001725        0.151732         0.000000  \n",
       "78         0.005764        0.172549         0.000000  \n",
       "90         0.013002        0.206913         0.011651  \n",
       "9          0.005660        0.089204         0.022835  \n",
       "96         0.007905        0.192382         0.004003  \n",
       "51         0.004457        0.126381         0.014014  \n",
       "71         0.001946        0.168647         0.005462  \n",
       "54         0.002006        0.182790         0.003379  \n",
       "37         0.006049        0.124383         0.011002  \n",
       "29         0.009257        0.110913         0.003440  \n",
       "16         0.003043        0.188643         0.000000  \n",
       "52         0.011290        0.190798         0.004212  \n",
       "13         0.006729        0.055838         0.065750  \n",
       "6          0.002233        0.176617         0.001988  \n",
       "4          0.010118        0.183703         0.001047  \n",
       "0          0.006111        0.189538         0.002545  \n",
       "39         0.015306        0.164361         0.001788  \n",
       "59         0.003170        0.105641         0.013332  \n",
       "2          0.008494        0.186186         0.001082  \n",
       "17         0.001474        0.076922         0.090245  \n",
       "19         0.008057        0.070134         0.018851  \n",
       "15         0.009199        0.066043         0.092846  \n",
       "\n",
       "[120 rows x 25 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.91      0.81       505\n",
      "        1.0       0.77      0.46      0.58       310\n",
      "\n",
      "avg / total       0.75      0.74      0.72       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, gridsearch.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('dimensions', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'classifier__n_estimators': [500], 'classifier__max_depth': [2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch = GridSearchCV(pipeline, {\n",
    "    #'dimensions': [PCA(), Pipeline([])],\n",
    "    'classifier__n_estimators': [500],\n",
    "    'classifier__max_depth': [2],\n",
    "    #'classifier__learning_rate': [0.99, 0.3, 0.1],\n",
    "    #'classifier__subsample': [1.0, 0.3],\n",
    "    #'classifier__loss': ['deviance', 'exponential']\n",
    "}, scoring='f1', cv=5, refit=True, n_jobs=4, verbose=1)\n",
    "\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False),\n",
      " 'dimensions': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pprint(gridsearch.best_estimator_.named_steps)\n",
    "df = pd.DataFrame(gridsearch.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.340234</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.545014</td>\n",
       "      <td>0.709821</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__max_depth': 2, 'classifier__n_es...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.271756</td>\n",
       "      <td>0.703117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636086</td>\n",
       "      <td>0.717774</td>\n",
       "      <td>0.609009</td>\n",
       "      <td>0.708975</td>\n",
       "      <td>0.631884</td>\n",
       "      <td>0.70582</td>\n",
       "      <td>0.303594</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.138292</td>\n",
       "      <td>0.005251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       4.340234          0.00625         0.545014          0.709821   \n",
       "\n",
       "  param_classifier__max_depth param_classifier__n_estimators  \\\n",
       "0                           2                            500   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'classifier__max_depth': 2, 'classifier__n_es...                1   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.271756            0.703117       ...                  0.636086   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.717774           0.609009            0.708975   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.631884             0.70582      0.303594        0.007655   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.138292         0.005251  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.93      0.81       505\n",
      "        1.0       0.79      0.39      0.52       310\n",
      "\n",
      "avg / total       0.74      0.73      0.70       815\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('dimensions', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
